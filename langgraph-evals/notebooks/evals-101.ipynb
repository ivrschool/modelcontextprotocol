{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d5ba5acc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import dotenv\n",
    "\n",
    "dotenv.load_dotenv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d45e1157",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langsmith import Client\n",
    "\n",
    "client = Client()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "fc63be30",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Programmatically create a dataset in LangSmith\n",
    "# For other dataset creation methods, see:\n",
    "# https://docs.smith.langchain.com/evaluation/how_to_guides/manage_datasets_programmatically\n",
    "# https://docs.smith.langchain.com/evaluation/how_to_guides/manage_datasets_in_application\n",
    "try:\n",
    "    dataset = client.create_dataset(\n",
    "        dataset_name=\"Sample dataset\", description=\"A sample dataset in LangSmith.\", \n",
    "    )\n",
    "except Exception as e:\n",
    "    print(e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "9f0f5632",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = client.read_dataset(dataset_name=\"Sample dataset\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "226a8a65",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create examples\n",
    "examples = [\n",
    "    {\n",
    "        \"inputs\": {\"question\": \"Which country is Mount Kilimanjaro located in?\"},\n",
    "        \"outputs\": {\"answer\": \"Mount Kilimanjaro is located in Tanzania.\"},\n",
    "    },\n",
    "    {\n",
    "        \"inputs\": {\"question\": \"What is Earth's lowest point?\"},\n",
    "        \"outputs\": {\"answer\": \"Earth's lowest point is The Dead Sea.\"},\n",
    "    },\n",
    "]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "4dca681e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'example_ids': ['6fa16cb3-e4ab-476d-883f-865638c23aca',\n",
       "  '08d88c01-713c-469f-b057-394a3c624c73'],\n",
       " 'count': 2}"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Add examples to the dataset\n",
    "client.create_examples(dataset_id=dataset.id, examples=examples)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "f71d6518",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langsmith import wrappers\n",
    "from openai import OpenAI\n",
    "import os\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "321c5485",
   "metadata": {},
   "outputs": [],
   "source": [
    "# os.getenv(\"GEMINI_API_KEY\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "0b149f46",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Wrap the OpenAI client for LangSmith tracing\n",
    "gemini_client = wrappers.wrap_openai(OpenAI(\n",
    "                                        api_key=os.getenv(\"GEMINI_API_KEY\"),\n",
    "                                        base_url=\"https://generativelanguage.googleapis.com/v1beta/\",\n",
    "                                        \n",
    "                                    ))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "25f8013b",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_client= OpenAI(\n",
    "                                        api_key=os.getenv(\"GEMINI_API_KEY\"),\n",
    "                                        base_url=\"https://generativelanguage.googleapis.com/v1beta/\"\n",
    "                                    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "37a98aec",
   "metadata": {},
   "outputs": [],
   "source": [
    "# response = test_client.chat.completions.create(\n",
    "#     model=\"gemini-2.5-flash\",\n",
    "#     messages=[\n",
    "#         {\"role\": \"system\", \"content\": \"You are a helpful assistant.\"},\n",
    "#         {\n",
    "#             \"role\": \"user\",\n",
    "#             \"content\": \"Explain to me how AI works\"\n",
    "#         }\n",
    "#     ]\n",
    "# )\n",
    "\n",
    "# print(response.choices[0].message)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "e3da9735",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the application logic you want to evaluate inside a target function\n",
    "# The SDK will automatically send the inputs from the dataset to your target function\n",
    "def target(inputs: dict) -> dict:\n",
    "    response = gemini_client.chat.completions.create(\n",
    "        model=\"gemini-2.0-flash-lite\",\n",
    "        messages=[\n",
    "            {\"role\": \"system\", \"content\": \"Answer the following question accurately\"},\n",
    "            {\"role\": \"user\", \"content\": inputs[\"question\"]},\n",
    "        ],\n",
    "    )\n",
    "    return { \"answer\": response.choices[0].message.content.strip() }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "8d78fca7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# target(inputs={\"question\": \"What is the capital of France?\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "71575853",
   "metadata": {},
   "outputs": [],
   "source": [
    "from openevals.llm import create_llm_as_judge\n",
    "from openevals.prompts import CORRECTNESS_PROMPT\n",
    "\n",
    "def correctness_evaluator(inputs: dict, outputs: dict, reference_outputs: dict):\n",
    "    evaluator = create_llm_as_judge(\n",
    "        prompt=CORRECTNESS_PROMPT,\n",
    "        model=\"google_genai:gemini-2.0-flash-lite\",\n",
    "        feedback_key=\"correctness\",\n",
    "    )\n",
    "    eval_result = evaluator(\n",
    "        inputs=inputs,\n",
    "        outputs=outputs,\n",
    "        reference_outputs=reference_outputs\n",
    "    )\n",
    "    return eval_result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "7a38d8fe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "You are an expert data labeler evaluating model outputs for correctness. Your task is to assign a score based on the following rubric:\n",
      "\n",
      "<Rubric>\n",
      "  A correct answer:\n",
      "  - Provides accurate and complete information\n",
      "  - Contains no factual errors\n",
      "  - Addresses all parts of the question\n",
      "  - Is logically consistent\n",
      "  - Uses precise and accurate terminology\n",
      "\n",
      "  When scoring, you should penalize:\n",
      "  - Factual errors or inaccuracies\n",
      "  - Incomplete or partial answers\n",
      "  - Misleading or ambiguous statements\n",
      "  - Incorrect terminology\n",
      "  - Logical inconsistencies\n",
      "  - Missing key information\n",
      "</Rubric>\n",
      "\n",
      "<Instructions>\n",
      "  - Carefully read the input and output\n",
      "  - Check for factual accuracy and completeness\n",
      "  - Focus on correctness of information rather than style or verbosity\n",
      "</Instructions>\n",
      "\n",
      "<Reminder>\n",
      "  The goal is to evaluate factual correctness and completeness of the response.\n",
      "</Reminder>\n",
      "\n",
      "<input>\n",
      "{inputs}\n",
      "</input>\n",
      "\n",
      "<output>\n",
      "{outputs}\n",
      "</output>\n",
      "\n",
      "Use the reference outputs below to help you evaluate the correctness of the response:\n",
      "\n",
      "<reference_outputs>\n",
      "{reference_outputs}\n",
      "</reference_outputs>\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(CORRECTNESS_PROMPT)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "efd2a49a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/pankajchandravanshi/mcp/langgraph-evals/.venv/lib/python3.13/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "View the evaluation results for experiment: 'first-eval-in-langsmith-c3604d58' at:\n",
      "https://smith.langchain.com/o/cb35aec6-2b3b-4044-bfca-43dbfad00211/datasets/c941b0f0-af2b-4f0b-9027-bd9a50339df7/compare?selectedSessions=db18717f-996b-4383-8b3e-848cf3ce599b\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0it [00:00, ?it/s]Key 'additionalProperties' is not supported in schema, ignoring\n",
      "Key 'parameters' is not supported in schema, ignoring\n",
      "1it [00:02,  2.36s/it]Key 'additionalProperties' is not supported in schema, ignoring\n",
      "Key 'parameters' is not supported in schema, ignoring\n",
      "2it [00:04,  2.41s/it]\n"
     ]
    }
   ],
   "source": [
    "# After running the evaluation, a link will be provided to view the results in langsmith\n",
    "experiment_results = client.evaluate(\n",
    "    target,\n",
    "    data=\"Sample dataset\",\n",
    "    evaluators=[\n",
    "        correctness_evaluator,\n",
    "        # can add multiple evaluators here\n",
    "    ],\n",
    "    experiment_prefix=\"first-eval-in-langsmith\",\n",
    "    max_concurrency=0,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc8180ac",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<ExperimentResults first-eval-in-langsmith-c3604d58>\n"
     ]
    }
   ],
   "source": [
    "print(experiment_results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "2cd2541c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<ExperimentResults first-eval-in-langsmith-c3604d58>"
      ],
      "text/plain": [
       "<ExperimentResults first-eval-in-langsmith-c3604d58>"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "experiment_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "2ff6941f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "View the evaluation results for experiment: 'large-boat-84' at:\n",
      "https://smith.langchain.com/o/cb35aec6-2b3b-4044-bfca-43dbfad00211/datasets/c941b0f0-af2b-4f0b-9027-bd9a50339df7/compare?selectedSessions=11908f19-0dbc-4099-a700-6ac2291ed051\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2it [00:00,  5.35it/s]\n"
     ]
    }
   ],
   "source": [
    "from langsmith import evaluate\n",
    "\n",
    "def correct(outputs: dict, reference_outputs: dict) -> bool:\n",
    "    \"\"\"Check if the answer exactly matches the expected answer.\"\"\"\n",
    "    return outputs[\"answer\"] == reference_outputs[\"answer\"]\n",
    "\n",
    "def dummy_app(inputs: dict) -> dict:\n",
    "    return {\"answer\": \"hmm i'm not sure\", \"reasoning\": \"i didn't understand the question\"}\n",
    "\n",
    "results = evaluate(\n",
    "    dummy_app,\n",
    "    data=\"Sample dataset\",\n",
    "    evaluators=[correct],\n",
    "    max_concurrency=2,\n",
    ")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "3366f899",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'large-boat-84'"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results.experiment_name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "0484c36b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['experiment_name', 'to_pandas', 'wait']\n"
     ]
    }
   ],
   "source": [
    "print([k for k in dir(results) if not k.startswith('_')])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "4a61f52e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>inputs.question</th>\n",
       "      <th>outputs.answer</th>\n",
       "      <th>outputs.reasoning</th>\n",
       "      <th>error</th>\n",
       "      <th>reference.answer</th>\n",
       "      <th>feedback.correct</th>\n",
       "      <th>execution_time</th>\n",
       "      <th>example_id</th>\n",
       "      <th>id</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>What is Earth's lowest point?</td>\n",
       "      <td>hmm i'm not sure</td>\n",
       "      <td>i didn't understand the question</td>\n",
       "      <td>None</td>\n",
       "      <td>Earth's lowest point is The Dead Sea.</td>\n",
       "      <td>False</td>\n",
       "      <td>0.001298</td>\n",
       "      <td>08d88c01-713c-469f-b057-394a3c624c73</td>\n",
       "      <td>a933f7cd-27e6-4b26-88ec-8e26e13ea1ab</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Which country is Mount Kilimanjaro located in?</td>\n",
       "      <td>hmm i'm not sure</td>\n",
       "      <td>i didn't understand the question</td>\n",
       "      <td>None</td>\n",
       "      <td>Mount Kilimanjaro is located in Tanzania.</td>\n",
       "      <td>False</td>\n",
       "      <td>0.008987</td>\n",
       "      <td>6fa16cb3-e4ab-476d-883f-865638c23aca</td>\n",
       "      <td>fc4cd304-23a3-4865-81e6-7a2c08f374bc</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                  inputs.question    outputs.answer  \\\n",
       "0                   What is Earth's lowest point?  hmm i'm not sure   \n",
       "1  Which country is Mount Kilimanjaro located in?  hmm i'm not sure   \n",
       "\n",
       "                  outputs.reasoning error  \\\n",
       "0  i didn't understand the question  None   \n",
       "1  i didn't understand the question  None   \n",
       "\n",
       "                            reference.answer  feedback.correct  \\\n",
       "0      Earth's lowest point is The Dead Sea.             False   \n",
       "1  Mount Kilimanjaro is located in Tanzania.             False   \n",
       "\n",
       "   execution_time                            example_id  \\\n",
       "0        0.001298  08d88c01-713c-469f-b057-394a3c624c73   \n",
       "1        0.008987  6fa16cb3-e4ab-476d-883f-865638c23aca   \n",
       "\n",
       "                                     id  \n",
       "0  a933f7cd-27e6-4b26-88ec-8e26e13ea1ab  \n",
       "1  fc4cd304-23a3-4865-81e6-7a2c08f374bc  "
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "results.to_pandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "104113c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from langgraph.graph import StateGraph, START, END, MessagesState\n",
    "from typing_extensions import TypedDict\n",
    "\n",
    "from langchain_google_genai import ChatGoogleGenerativeAI\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv()\n",
    "\n",
    "llm = ChatGoogleGenerativeAI(model=\"gemini-2.5-flash\")\n",
    "\n",
    "# Define input schema\n",
    "class InputState(TypedDict):\n",
    "    question: str\n",
    "\n",
    "# Define output schema\n",
    "class OutputState(TypedDict):\n",
    "    answer: str\n",
    "\n",
    "# Combine input and output\n",
    "class OverallState(InputState, OutputState):\n",
    "    pass\n",
    "\n",
    "\n",
    "def chat_node(state: InputState):\n",
    "\n",
    "    question =  state[\"question\"]\n",
    "    \n",
    "    answer = llm.invoke(question)\n",
    "    return {\"answer\": answer.content, \"question\": state[\"question\"]}\n",
    "\n",
    "graph_builder = StateGraph(OverallState, input_schema=InputState, output_schema=OutputState)\n",
    "\n",
    "graph_builder.add_node(\"chat\", chat_node)\n",
    "\n",
    "graph_builder.set_entry_point(\"chat\")\n",
    "graph_builder.set_finish_point(\"chat\")\n",
    "\n",
    "relationship_graph = graph_builder.compile()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
